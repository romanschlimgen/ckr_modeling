{
  "articles": [
    {
      "path": "index.html",
      "title": "CKR Modeling",
      "author": [],
      "contents": "\nCXCR4 - CXCL12\nInteractive model shown below\n\n\n\n\n",
      "last_modified": "2024-04-03T18:25:14-05:00"
    },
    {
      "path": "models.html",
      "title": "Patterns and anti-patterns of data analysis reuse",
      "description": "A speed-run through four stages of data analysis reuse, to the end game you probably guessed was coming.\n",
      "author": [
        {
          "name": "Miles McBain",
          "url": "https://milesmcbain.com"
        }
      ],
      "date": "2024-03-11",
      "contents": "\n\n\n\n\n\n\n\n\nChemokine Modeling Database\n\n\nHome\n\n\nModels\n \n▾\n\n\nmodels\ncoming soon\n\n\n\n\nPharmacology\n \n▾\n\n\ncoming soon\ncoming soon\n\n\n\n\nContact Us\n \n▾\n\n\ncoming soon\ncoming soon\n\n\n☰\n\n\n  \n    \n      \n        \n        \n        \n      \n      Chemokine Modeling Database\n    \n    \n      \n  Home\n\n\n  \n    Models\n     \n    \n  \n  \n      models\n    \n    \n      coming soon\n    \n  \n\n  \n    Pharmacology\n     \n    \n  \n  \n      coming soon\n    \n    \n      coming soon\n    \n  \n\n  \n    Contact Us\n     \n    \n  \n  \n      coming soon\n    \n    \n      coming soon\n    \n  \n      \n  \n\n\n\n\n\n\nPatterns and anti-patterns of data analysis\nreuse\nMiles McBain\n2024-03-11\n\n\n\n\nEvery data analysis / data scientist role I’ve worked in has had a\nstrong theme of redoing variations of the same analysis. I expect this\nis something of an industry-wide trend. If you’re in marketing you’re\nranking prospects, and A/B testing their responses. If you’re in\nemergency services planning you’re re-running the same kinds of\nsimulations targeting the same kinds of KPIs (response times). If you’re\nin banking you’re predicting churn, arrears etc. The details change, but\nyou see the same problem structure repeated over and over.\nEven when I was supporting researchers I often saw them cut and paste\nmethods from older works and ‘adapt them’ to whatever the current\nquestion was, using their favoured hammer to hit every nail.\nIt’s The Data Analyst’s Curse: You’re keen to prove you can\nuse these much-hyped skills to add value. But as soon as you do, you’re\nstuck. Your value-add becomes business as usual (BAU). And good luck to\nyou if you think you’re going to shirk BAU to investigate more\ninteresting data - that would be a value-subtract. And those make the\nwell dressed people with soft-keyboard-covered-iPads under their arms\nvery unhappy.\nGiven your curse, your sanity now depends on re-using as much work as\npossible across these achingly-samey-but-subtlety-different projects.\nYou and your team need to set yourselves up so you can re-cover old\nground quickly, hopefully ensuring that you have plenty of time to build\non capabilities you’ve already proven, rather than just executing and\nre-executing the same capability until you all get bored and quit.1\nSo now lets look at the ways you can reuse your data analysis work.\nAnd I’m making two pretty big assumptions in this discussion:\nYour work is written in code e.g. R, Python, Julia, Rust etc. - If\nyou’re using PowerBI, or god forbid Excel, you’re largely limited to the\nfirst two less than ideal approaches I am about to discuss.\nYou use a technology like Quarto,\nor Rmarkdown, or Shiny, such that any document-like\nproducts are generated from code. - If you don’t do this there’s a host\nof added pitfalls on top of what we’ll discuss here, more than can\nsensibly be covered in one blog post.\n\nCopying and pasting from stuff you did earlier\nIt’s in the name. Instead of redoing an analysis you know how to do\nfrom scratch you open up the project from last time and go ctrl-c ctrl-v\nwild. At first this may feel pretty good. Almost like cheating. If\nyou’re clever you won’t tell your boss or clients how much time you just\nsaved. And not because you’re dishonest. No it’s because what the ol’\ncopypasta is actually doing is slowly accumulating a mountain of\n‘technical-debt’ or\n‘old-weird-stuff-we-don’t-really-understand-anymore-but-keeps-coming-along-for-the-ride-and-now-we-can’t-touch-it-without-ruining-everything’.\nEventually the interest on that technical debt, paid in terms of\n‘time spent stuffing around trying to add an incremental improvement’,\neats all the productivity gains you got from the copy-paste in the first\nplace. 2\nThere are several ways this can happen. Here’s a scenario.\nThe client really liked the dynamics evident in the event stream plot\non project BLUE. So for project ORANGE it’s only natural to pull that\nplot and its data processing dependencies in via copy-paste.\nBut hang on your colleague Russell is a fan of the work too. He wants\nto show it to a new client on project GREEN. No worries mate, it’s in\n/projects/GREEN/GREEN_final_complete2/code/script04.R.\nWhat? No it worked on my machine, I swear. Oh wait. Try the dev version\nof ggplot2.\nOh no. Murphy’s law. You found a bug in that beautiful plot you were\nso proud of. There were some pesky duplicates in the data. So you fixed\nit on project ORANGE. Better fix it on project BLUE too. And lookout you\nforgot to tell Russell on project GREEN!\nWhat kind of damage can we expect from this?\nCopy-pasted code is like a virus. With each reproduction it can\nacquire mutations that make it more or less fit to be copied in the\nfuture. Maybe Russell added a sweet forecast with uncertainty gradients\nto the plot. He didn’t bother to fix the duplicate issue because it was\nminor in his case. Now when someone wants a forecast, it is Russell’s\nvariant of your buggy code that will reproduce. After it acquires enough\nmutations you could very well accidentally copy a version containing\nyour original bug into your own project, and you’ll be confused as to\nhow this bug you fixed 18 months ago has risen from the grave.\nThe erosion of trust, the stress, the time wasted, the fumbling for\nthe right version in this every growing pile is the interest bill for\nyour technical debt.3\n\n\nThe one template to rule them all\nThe cure for the copy-paste virus and all it’s ills is\ncentralisation. For each project you build on a centralised base that\ncontains the definitive versions of all the commonly used functionality\nthat makes up your data analysis capability. Creating a shared project\ntemplate is a realisation of this idea. The crudest example of a viable\ntemplate might be a giant monolith of a Quarto file that contains all\nyour project code and text, with placeholder text or #TODO\ncomments that indicate where the data analyst needs to insert stuff that\nis specific to their current project.\n\nThe return of copy-paste\nIn theory, as the team builds on their data analysis capability, the\nshared template can be updated so all future projects will benefit from\nenhancements or fixes that are made. I say ‘in theory’ because the first\npitfall of this approach is that in order to build on the template you\nhave to take a copy of it. So changes you make to your copy don’t\nautomatically get reflected in the centralised version. Your colleagues\naren’t even aware your changes exist. There may be some kind of\nagreement in place that ‘we’ll all push our changes back to the central\ntemplate as we go’, but since this task is not necessary for the\ndelivery of the work, it relies on human discipline, or rigid adherence\nto process, which inevitably fails under pressure.\nSo despite the best of intentions your team can end up with diverging\ntemplates in play, and the temptation is to reuse a version with the\nfeatures you need rather then the shared version, and THUNK you’ve\nbackslid to ‘copying and pasting from stuff you did earlier’.\nBut that’ll never happen to your team right? You’re not that lazy.\nWell let me tell you about a different kind of trap templates hold for\nclever industrious types.\n\n\nPower overwhelming\nWhen you’re maintaining a template it seems like the productive thing\nto do should be to push as much as possible into it, to minimise the\nwork needed for each data analysis job. Perhaps you have an ideal where\nyou press a button, the machinery cranks away, and spits out all the\nmetrics, tables, and plots you need. Leaving you just bit of\ninterpretation and a few points of commentary to do.\nThe challenge you’ll face in getting there is that there are always\nniggling little details of each new data set, and new context that\nrequire some special handling. To try to solve for this you can make the\ntemplate configurable. You might even employ meta-programming techniques\nto conditionally change the way your outputs look.\nSo because the word ‘configure’ was spoken, someone will suggest\nusing YAML or JSON to store it. Guaranteed. Because that’s just what you\ndo config right? Resist this. I urge you. Does YAML look that much\nbetter to your eyes than a literal R list or Python dictionary? Code can\nbe refactored, code can be auto-completed, code can be analysed,\nchecked, styled etc. But text? May you be so lucky.\nOnce you’re talking template config you’re on a narrow and dangerous\npath. You feed the beast and it grows bigger and hungrier. Despite your\nbest efforts the template will never be flexible enough to satisfy your\nideal. You’ll find yourself saying ‘if we just allow X to be\nconfigurable…’ and field upon field will be added. Perhaps you’ll find\naway to allow configured objects to be referenced so they can be reused.\nOr maybe you’ll sprinkle a bit of flow control on it - ‘if this not\nthat’ etc.\nOr maybe to speed up the now arduous config process you’ll template\nthe config for the template, yeah that should probably do it. No? Okay\nfuck it. Maybe you’ll allow the user to supply custom code expressions\nenclosed in \" (of course) to be inserted at critical\njunctures in the project. You may think I am exaggerating but these are\nhorrors I have seen with my own eyes. Some created by my own hand.\nAnd maybe if you’re fortunate, when you’re bloodied and panting,\nhaving just screamed “ARE YOU NOT ENTERTAINED?” into your screen like a\nlunatic, you will have a moment of clarity, and you will realise that\nyou were never configuring anything. You were slowly imbuing your\nconfiguration with the powers of code to create yourself a new high\nlevel, fully functional, but horribly specified language with which to\ndescribe your solution to the data analysis problem. It was programming\nall along.\nIf you are at this point now do not despair. Many have been here\nbefore you. I know this phenomenon as The Inner\nPlatform Effect and I learned about it the hard way in my first\njob. I also like to remix Greenspun’s\ntenth rule of programming for this occasion:\nAny sufficiently complicated plain text configuration scheme contains an ad\nhoc, informally-specified, bug-ridden, slow implementation of half of\nCommon Lisp.\nTemplates are, at best, a partial solution to data analysis reuse. In\na similar way to the copy-paste strategy, they can feel very productive\ninitially, but if you let them become too complex, increasing amounts of\nresources will go to keeping the template machinery working, sucking\ntime away from developing your team’s data analysis capabilities.\n\n\n\nThe one package to rule them all\nInstead of expressing a high level project solution in template\nconfiguration, there’s a much better time to be had creating a library\nof powerful domain specific functions (or objects if that’s your thing)\nthat can be composed and invoked to perform major parts of the data\nanalysis. The flexibility to handle all the little individual\ncircumstantial curios comes from parameters which can change the\nbehaviour of functions, or from composing the functions in different\nways, perhaps adding, removing, or swapping them to account for\ndifferent situations.\nWith powerful functions you still maximise the ratio of work done to\nkeystrokes pressed. You do so by doing more work with each line of code\nyou type, as opposed to building a machine powered by config that\noutputs prefabricated analysis. And code, unlike configuration, has all\nthe degrees of freedom you could possibly need from the outset.\nSo assuming you can create this suite of functions designed to work\ntogether that the whole team will share and contribute to, it’s fairly\nnatural to put them into a software package or library. One reason for\nthis is that the combination of package name and version give you what\nyou need create reproducible work. The package can be updated, but so\nlong as you know the version number your project depends on, you can\ninstall that version from the source code repository, and your code\nwon’t break due to changes made by others.\nWhy is this important? Well I think one of the main reasons templates\nbackslide into copy-pasting your own bespoke versions of said templates\nis because it can feel like ‘there’s not enough time’ to understand what\n(if any) changes have been made to the centralised version by others,\nand how to incorporate them with your own. A package acts as a kind of\n‘forcing function’ for better practices that make this easier.\nAn example practice is automated testing. This is a practice\ntypically associated with packages, but not configurable templates. A\ntest suite can give your team the confidence to merge multiple streams\nof changes together, since a passing test suite becomes strong evidence\nthat the changes mesh together nicely!\nYou may be wondering about the overhead of building and maintaining\nsoftware packages. People who haven’t done much of it usually do. The\ntrick is that there are software packages that contain sets of powerful\nfunctions dedicated to creating and maintaining software packages. In R\nthese are {devtools}, {testthat}, {usethis} et. al. With these I can\ncreate a fully compliant and installable R package, backed by a git\nrepository, in less than 10 minutes. In most decent programming\nlanguages I have used this is also the case. Nothing exceptional going\non here.\nThat fact that the pattern of breaking a problem domain into powerful\nfunctions that are packaged into modular units can be recursively\napplied to solve the problem of creating packages of functions should be\nsome evidence that this is a scalable technique for managing complexity.\nAnd it has been complexity, in various guises, that has wrought havoc on\nthe reuse techniques mentioned up until this point.\n\nCaution: Oversized\nI’m going to warn you about a pitfall here, but unfortunately it\nprobably won’t save you. Every workplace I have seen turn toward\nsoftware packaging as a tool for data analysis reuse has fallen into\nthis pit. Perhaps we are all destined to.\nI surmise it’s to do the fact that initially creating a software\npackage can be daunting. There’s a bit to get your head around.\nConventions to learn. So since it seems like it’s going to be a slog,\nmaybe teams imagine that this is a task they’re going to undertake once.\nIf they work for ACME company then they’re making a single package:\n{aceme}, {acmefun}, {acemetools}, {acmeutils} or some such generic\nconcept like that.\nThe problem with this idea is that eventually complexity overcomes\nhere as well. One reason is that it’s generally considered bad juju to\nremove functions from a package. You typically do a thing called\n‘deprecation’ where you make calling the function throw threatening\nwarnings to stop using it. And you leave those warnings in place for a\nlong while before you hit delete, so that all the package’s users have a\nchance to update their project sources.\nSo if it’s easy to add, but hard to remove, package bloat is a risk.\nIn a bloated package it gets hard to find things. Reading the\ndocumentation becomes a chore. In their bloat-induced-ignorance people\nmay add functionality to it that is a rehash of stuff that already\nexists in a different form, further exacerbating this problem.\nI also believe monolithic packages encourage bad software design.\nAssumptions about the internal implementations of functions or the\ninternal data structure of their outputs may bleed out into other areas\nof the package, causing them to become coupled by those assumptions. In\na sprawling code base these kinds of couplings are not always clear, so\nin practice this means you run into unexpected challenges when rewriting\nthings to work better, since you unwittingly violate assumptions made\nelsewhere, making more work for yourself.\nBy contrast, in a modular system of tightly focused software\npackages, it is more obvious that knowledge relating to implementation\ndetails should not influence code across package boundaries. To do so\nwould imply the two packages have to be updated in a synchronised\nfashion, and that sounds like an awkward bad time, because it is.\n\n\nRelated: One function to rule them all\nI’ve seen a similar (anti)pattern to ‘one-package’ play out with\nfunctions. This seems to catch out people who are transitioning out of a\nscript-based workflow with lots of duplicated code, into a style based\non functional composition.\nMassive functions get written with perhaps a dozen arguments,\ncontaining hundreds of lines of code, that are really not much more than\nwrangle_data.script rebadged as:\nwrangle_data(\n  x, \n  y, \n  z, \n  cubits, \n  flotsits, \n  n_widgets, \n  calibration_factor,\n  tolerance, \n  initial_state, \n  start_date, \n  end_date, \n  output_path\n  )\nSure we’re using functions, but we’re actually attempting to template\nthe entire solution using the function’s arguments. Many of the pitfalls\nof templates apply 4.\nSuch a function is pretty much un-testable due to the combinatoric\nexplosion of possible sets of inputs. And you can bet that the internals\nare not written in high level domain-specific code. When I look at that\nfunction signature I hear the screams of matrices being ground to a fine\npowder of bits through the conical burrs of nested for loops, and\nlaboured index arithmetic.\nFunctions with these properties are hard to update with confidence,\nso they tend to set like concrete. Discrete sections of code that might\nbe more widely applicable are locked up inside, unless… you could\ncopy-paste them out? ;)\nAs per concrete pretty much the only way to fix these functions is to\nbreak them up do over (with modular re-usable pieces!).\n\n\n\nYour personal ’verse of packages\nWe’re getting to it now! When we were discussing the ‘one-package’\npattern we talked about how the pattern of breaking down data analysis\ncode into reusable functions stored in packages is a recursively\napplicable method for managing complexity in reusable code - that is we\ncan use the same idea to break down both analysis projects and\nmonolithic packages into a modular ‘universe’ of packages that are\ninteroperable.\nA healthy universe will likely contain packages spread out along the\n‘ladder of abstraction’ - that is we’ll have a mix of user-facing tools\n(high level), and lower level packages that contain common code that\npowers those tools (low level).\nExamples are probably useful here. In my current job we have a small\npackage called {checkurself} that facilitates creating and\nrunning list of quality checks on a dataset. This package can be used\ndirectly, but it shows up as a dependency in a higher level package\ncalled {tdctargets} which contains macros5 for pipeline\nconstruction. There are macros that define certain kinds of pipeline\ntargets (inputs / outputs) that are quality checked, with the quality\ncheck results automatically becoming a separate pipeline target. You can\nroll with a default quality check, or you can use\n{checkurself} directly to plug in your own.\nOther examples of lower level packages in your universe might be\nthings that wrap up API endpoints or SQL queries to provide easy access\nto commonly used source data. These might show up as dependencies for\nhigher level packages that make frequently used visualisations of those\ndata.\nIn my last job we were endlessly plotting fire stations on maps. So\nit made sense build out tooling to the point where you could just do\nsomething like:\nqfesdata::get_station_locations() |>\n  ggplot() +\n  snapbox::layer_mapbox(geometry = st_bbox(area_of_interest)) +\n  qfesvis::layer_stations() +\n  # some kind of analysis layer\n{qfesdata}, {snapbox}, and\n{qfesvis} were the in-house built packages that wrapped up\na surprisingly large amount stuff: database queries, API calls,\ngeospatial data transformation, plotting aesthetic mappings etc. Making\nbeautiful maps went from hours down to minutes.\nWe also retained the flexibility to build off in new directions on\ntop of what we had, without the weight of copying hundreds of lines of\ncode around.\nOn the subject of identifying niches for internal packages, I really\nenjoyed Emily\nRiederer’s essay on the topic, which introduces the idea that\ninternally developed packages are kind of like extra expert team\nmembers.\n\nYou can give your software a graceful death\nWith the package universe, the problem of ‘easy to add difficult to\nremove’ that drives individual package bloat is a bit less severe. If a\npackage becomes superseded or irrelevant, you can just stop taking a\ndependency on it, and forget about it. Unlike deprecated functions that\nhave to live on guarded by warnings, you’re not going to be constantly\nreminded of a superseded package. You won’t have to navigate around its\nsource and documentation when they show up in your searches.6\n\n\nSnap back to reality\nI’ve presented a shared universe of domain specific software packages\nas an idealised solution to data analysis code reuse. There are several\nproblems with this idea in practice.\nThe first is how and when do you actually build out the universe?\nTypically you can’t design this type of thing up front. You might know\nfrom experience that you’re certain to need a few pieces, but there will\nalways be niche domain specific tasks that you do not recognise as\ncommon components of the workflow until you’ve actually performed the\nsame flavour of data analysis a few times.\nThe implication here is that you’re going to need to adopt a\n‘crafts-person’ style approach, where your tools are continually honed,\nand their effectiveness at producing superior results continuously\nexamined. This should lead to feedback that improves the tooling,\ncreating the headroom on top of ‘business as usual’ that you can use to\nimprove the tooling in the future. It’s a virtuous cycle.\nIt’s probable you’re going to have to start this virtuous cycle from\nless than virtuous, or ‘humble’, beginnings. And you should be fine with\nthat. The important thing is to keep applying the pattern of breaking\ndown the work into powerful domain specific modular tools. I always\nidealise this process like a ratchet. Ideally the when the team ‘clicks\nforward’ we lock in capability in a way that can’t be wound back.\nTo lock in capability the tooling has to be well designed and built.\nSo practices like ‘dogfooding’7, seeking peer feedback early, formal code\nreview, and software testing become highly relevant to making this\nhappen.\nThe second problem is that it’s all well and good to have this\nuniverse of powerful tools at your disposal, but starting a known type\nof data analysis from scratch every time, typing out the same\nboilerplate code to load your tools, and ingest the data can feel like a\nbit of a chore.\nThis is where a little templating can go a long way. My advice is to\navoid building the complete machine (no config!), and rather conceive a\nlightweight project skeleton, that includes boiler plate code, and\nimplies the overall project structure8, but leaves the user to flesh out the\n(hopefully) interesting parts.\n\n\n\nConclusion\nI presented four patterns for reusing data analysis code:\nCopy-paste from stuff you did earlier\nA Centralised template\nA shared software package\nA shared universe of software packages\nAnother way of viewing this sequence is as a kind of march of\nprogress like this:\nEvery data analysis team I have worked on has gone through this\nmarch, although some were already more advanced when I joined. I am not\nsure if it’s possible to skip evolutions. It it may only be possible to\n‘speed run’ our teams through them.\nThings being increasingly ‘centralised’ or ‘shared’ was a common\nthread, but I have completely neglected cultural challenges that can\narise with doing this. Suffice to say for now: establishing and\nmaintaining trust and shared values among the team is a necessary\ncondition for effective reuse of shared resources, and progressing\nthrough the stages.\nPerhaps you have seen teams even further along this march than I\nhave. I’d be eager to hear what lies beyond!\nPresented with thanks to Nick Tierney for reviewing this\npost. It runs with some ideas from a talk I\ngave in 2022.\n\n\nThe sort of mind that will tolerate the guts of\ncomputers, and the various ways they can torture data, does so out of a\nkind of insatiable curiosity. That curiosity can be harnessed to great\nproductive effect. But remove the opportunity to be curious, and\ncuriosity will vest elsewhere, and morale will suffer.↩︎\nThe interest on technical debt is debited at the least\nopportune time.↩︎\nPerhaps you can see why if you don’t use something like\nQuarto to generate your documents, you increase the surface area for\n‘copy-paste gone wrong’ type mistakes to occur on greatly, since you\nneed to execute synchronised copy-pastes in source code and document\nfiles.↩︎\nIf a function starts taking YAML configuration files as\narguments you are on borrowed time.↩︎\n Okay in R, they’re ‘target factories’ that depend on\nR’s special flavour of meta-programming.↩︎\nIn reality you probably are because you’ll have that one\ncolleague that will cling to it.↩︎\nthat is ‘eating your own dogfood’ or using your own\ntools to understand their strengths and weaknesses↩︎\nIt’s a little appreciated fact that deciding how to\nstructure things is actually the ‘deciding how to name things’ problem\nraised to a power↩︎\n\n\n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2024-04-03T15:35:06-05:00"
    }
  ],
  "collections": ["posts/posts.json"]
}
